{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9f6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample text for sentiment analysis\n",
    "text = \"one man is in the home. He is waiting to kiss his wife\"\n",
    "\n",
    "# Analyze the sentiment of the text\n",
    "sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "# Determine sentiment based on the compound score\n",
    "compound_score = sentiment_scores['compound']\n",
    "\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = 'Positive'\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = 'Negative'\n",
    "else:\n",
    "    sentiment = 'Neutral'\n",
    "\n",
    "# Display the sentiment and scores\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Positive Score:\", sentiment_scores['pos'])\n",
    "print(\"Negative Score:\", sentiment_scores['neg'])\n",
    "print(\"Neutral Score:\", sentiment_scores['neu'])\n",
    "print(\"Compound Score:\", sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3829a5a",
   "metadata": {},
   "source": [
    "# 1. NLTK\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "Sentence Tokenization:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"This is a sample sentence. And here's another one.\"\n",
    "sentences = sent_tokenize(text)\n",
    "Word Tokenization:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Tokenize this sentence into words.\"\n",
    "words = word_tokenize(text)\n",
    "2. Part-of-Speech Tagging:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import pos_tag\n",
    "words = word_tokenize(\"This is an example sentence.\")\n",
    "tagged_words = pos_tag(words)\n",
    "3. Named Entity Recognition (NER):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import ne_chunk\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "named_entities = ne_chunk(tagged_words)\n",
    "4. Sentiment Analysis:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sia.polarity_scores(\"I love this product!\")\n",
    "5. Text Classification:\n",
    "\n",
    "Assuming you have a labeled dataset, you can train and evaluate a classifier using NLTK. For instance, a Naive Bayes classifier:\n",
    "python\n",
    "Copy code\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "training_data = [(feature_set, label) for feature_set, label in your_training_data]\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "6. WordNet Integration:\n",
    "\n",
    "Access synonyms and antonyms using WordNet in NLTK:\n",
    "python\n",
    "Copy code\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = wordnet.synsets(\"happy\")\n",
    "antonyms = wordnet.synsets(\"happy\", antonyms=True)\n",
    "7. Frequency Distribution:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import FreqDist\n",
    "words = word_tokenize(\"This is a simple example sentence.\")\n",
    "freq_dist = FreqDist(words)\n",
    "8. Regular Expressions:\n",
    "\n",
    "You can use Python's regular expressions in combination with NLTK for more advanced text processing. For example, to find all capitalized words:\n",
    "python\n",
    "Copy code\n",
    "import re\n",
    "text = \"Find All Capitalized Words in This Text.\"\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "9. Text Summarization:\n",
    "\n",
    "NLTK provides some tools for extracting sentences based on criteria like sentence importance, which can be used in text summarization. However, more advanced summarization techniques may require additional libraries or algorithms.\n",
    "Please note that the above snippets provide a basic introduction to these NLP tasks using NLTK. Depending on your specific use case and requirements, you may need to extend these examples or explore other NLTK functions and modules in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5298780",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text =\"how are you doing? I was waiting for you. I will be going soon\"\n",
    "sent = sent_tokenize(text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ada95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"how are you doing? I was waiting for you. I will be going soon\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc72114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee834806",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "named_entities = ne_chunk(tagged_words)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cc989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "training_data = [(feature_set, label) for feature_set, label in your_training_data]\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "print(classifier)\n",
    "NameError: name 'your_training_data' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Find synonyms for the word \"happy\"\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"sad\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "# Find antonyms for the synonyms\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "            antonyms.append(antonym.name())\n",
    "\n",
    "print(\"Synonyms:\", set(synonyms))\n",
    "print(\"Antonyms:\", set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15ef791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize  # Import word_tokenize from nltk.tokenize\n",
    "\n",
    "text = \"This is a simple example sentence.\"\n",
    "words = word_tokenize(text)  # Tokenize the text\n",
    "freq_dist = FreqDist(words)  # Create a frequency distribution\n",
    "print(words)\n",
    "print(freq_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba67ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np  # Import NumPy\n",
    "\n",
    "def extractive_summarization(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    \n",
    "    # Convert the 'scores' array to a 1D array or list\n",
    "    scores = np.array(tfidf_matrix.sum(axis=1)).ravel()  # Using NumPy 'ravel()'\n",
    "\n",
    "    # Alternatively, you can use 'tolist()' to convert to a list:\n",
    "    # scores = tfidf_matrix.sum(axis=1).tolist()\n",
    "\n",
    "    ranked_sentences = [sentences[i] for i in scores.argsort()[-num_sentences:]]\n",
    "    summary = ' '.join(ranked_sentences)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text = \"Integrated Cost-Benefit Analysis (CBA) combines financial and non-financial aspects, such as social and environmental impacts, to provide a holistic assessment of investment opportunities. Green Investment Appraisal (GIA) is another emerging approach that explicitly incorporates sustainability criteria into investment decisions. \"\n",
    "summary = extractive_summarization(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ef06e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Greeting responses\n",
    "greetings = [\"hello\", \"hi\", \"hey\", \"howdy\", \"greetings\"]\n",
    "# Responses\n",
    "responses = [\"Hello!\", \"Hi there!\", \"Greetings!\", \"Hey!\", \"Hello, how can I help you today?\"]\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "# Function to respond to user input\n",
    "def respond(user_input):\n",
    "    user_words = lemmatize(word_tokenize(user_input.lower()))\n",
    "    for word in user_words:\n",
    "        if word in greetings:\n",
    "            return random.choice(responses)\n",
    "    return \"I'm sorry, I don't understand.\"\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = respond(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1f5ed55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c21df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'C:\\\\Users\\\\anas_\\\\OneDrive\\\\Python_Backup'  # Replace with the actual path to your corpus directory\n",
    "corpus = PlaintextCorpusReader(corpus_root, 'document1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e11f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document1.txt']\n"
     ]
    }
   ],
   "source": [
    "file_ids = corpus.fileids()\n",
    "print(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76656e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content = corpus.raw('document1.txt')\n",
    "document_words = corpus.words('document1.txt')\n",
    "document_sentences = corpus.sents('document1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4761701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 167), ('the', 132), ('.', 118), ('a', 83), ('I', 64), ('to', 60), ('in', 52), ('was', 48), ('of', 43), ('and', 41), ('’', 27), ('on', 21), ('that', 21), ('from', 20), ('were', 20), ('we', 15), ('or', 15), ('it', 13), ('by', 12), ('had', 12), ('into', 11), ('for', 11), ('more', 11), ('d', 11), ('all', 10), ('when', 9), ('at', 9), ('t', 9), ('up', 8), ('our', 8), ('way', 8), ('Beirut', 8), ('The', 8), ('In', 8), ('Then', 8), ('as', 8), ('where', 8), ('?', 8), ('an', 8), ('one', 7), ('see', 7), ('time', 7), ('my', 7), ('might', 7), ('you', 7), ('few', 7), ('people', 7), ('old', 7), ('There', 6), ('which', 6), ('cars', 6), ('nearly', 6), ('car', 6), ('with', 6), ('s', 6), ('could', 6), ('like', 6), ('this', 6), ('place', 6), ('go', 6), ('taxi', 5), ('We', 5), ('two', 5), ('someone', 5), ('road', 5), ('any', 5), ('their', 5), ('seemed', 5), ('out', 5), ('only', 5), ('other', 5), ('everyone', 5), ('knew', 5), ('around', 5), ('back', 5), ('her', 5), ('Middle', 4), ('East', 4), ('Lebanon', 4), ('Syria', 4), ('Saudi', 4), ('through', 4), ('then', 4), ('town', 4), ('men', 4), ('block', 4), ('every', 4), ('each', 4), (';', 4), ('have', 4), ('struck', 4), ('city', 4), ('would', 4), ('blood', 4), ('power', 4), ('wanted', 4), ('passenger', 4), ('than', 4), ('It', 4), ('down', 4), ('get', 4), ('bomb', 4), ('On', 4), ('them', 4), ('been', 4), ('They', 4), ('what', 4), ('America', 4), ('trying', 4), ('“', 4), ('”', 4), ('said', 4), ('desert', 3), ('black', 3), ('upon', 3), ('drove', 3), ('hit', 3), ('another', 3), (':', 3), ('When', 3), ('began', 3), ('called', 3), ('streets', 3), ('plates', 3), ('became', 3), ('soldiers', 3), ('wasn', 3), ('daughter', 3), ('single', 3), ('who', 3), ('spot', 3), ('before', 3), ('roads', 3), ('first', 3), ('things', 3), ('drive', 3), ('be', 3), ('driving', 3), ('family', 3), ('country', 3), ('become', 3), ('war', 3), ('break', 3), ('begin', 3), ('needed', 3), ('help', 3), ('there', 3), ('just', 3), ('A', 3), ('radio', 3), ('left', 3), ('imagine', 3), ('couldn', 3), ('about', 3), ('Cottontail', 3), ('maybe', 3), ('me', 3), ('decided', 3), ('bar', 3), ('woman', 3), ('toward', 2), ('across', 2), ('Dammam', 2), ('among', 2), ('refugees', 2), ('Wild', 2), ('West', 2), ('day', 2), ('near', 2), ('took', 2), ('man', 2), ('canyons', 2), ('mountains', 2), ('north', 2), ('no', 2), ('sometimes', 2), ('realized', 2), ('point', 2), ('finally', 2), ('four', 2), ('protests', 2), ('spread', 2), ('went', 2), ('still', 2), ('Syrian', 2), ('started', 2), ('guns', 2), ('You', 2), ('school', 2), ('pass', 2), ('five', 2), ('next', 2), ('month', 2), ('border', 2), ('tiny', 2), ('building', 2), ('his', 2), ('wife', 2), ('wrecks', 2), ('miles', 2), ('half', 2), ('burst', 2), ('flames', 2), ('sea', 2), ('bright', 2), ('hours', 2), ('moved', 2), ('Assad', 2), ('year', 2), ('some', 2), ('driver', 2), ('if', 2), ('call', 2), ('never', 2), ('Arabia', 2), ('lived', 2), ('instead', 2), ('because', 2), ('feel', 2), ('killed', 2), ('money', 2), ('highway', 2), ('pedal', 2), ('site', 2), ('something', 2), ('not', 2), ('security', 2), ('quickly', 2), ('blast', 2), ('My', 2), ('during', 2), ('come', 2), ('Holiday', 2), ('Inn', 2), ('battle', 2), ('After', 2), ('rolled', 2), ('strong', 2), ('west', 2), ('To', 2), ('journey', 2), ('acknowledging', 2), ('guest', 2), ('everything', 2), ('wrong—that', 2), ('felt', 2), ('right', 2), ('night', 2), ('minutes', 2), ('simply', 2), ('ready', 2), ('faster', 2), ('take', 2), ('seat', 2), ('away', 2), ('shoe', 2), ('repair', 2), ('shop', 2), ('corner', 2), ('always', 2), ('threw', 2), ('beach', 2), ('between', 2), ('But', 2), ('That', 2), ('California', 2), ('behind', 2), ('door', 2), ('thought', 2), ('whorehouse', 2), ('parking', 2), ('lot', 2), ('mountain', 2), ('washed', 2), ('atlas', 2), ('explain', 2), ('options', 2), ('told', 2), ('looked', 2), ('suddenly', 2), ('caught', 2), ('yet', 2), ('Behind', 2), ('drift', 2), ('plastic', 2), ('champagne', 2), ('glasses', 2), ('pile', 2), ('approximately', 2), ('100', 2), ('tongue', 2), ('depressors', 2), ('hard', 2), ('working', 2), ('well', 2), ('anyone', 2), ('involved', 2), ('leaned', 2), ('head', 2), ('lap', 2), ('is', 2), ('long', 2), ('they', 2), ('she', 2), ('too', 2), ('Once', 1), ('Upon', 1), ('Time', 1), ('Nathan', 1), ('Deuel', 1), ('Driving', 1), ('Arabian', 1), ('Beirut—quickly', 1), ('becomes', 1), ('end', 1), ('hill', 1), ('sliced', 1), ('making', 1), ('riding', 1), ('strip', 1), ('asphalt', 1), ('lines', 1), ('drawn', 1), ('span', 1), ('tar', 1), ('wide', 1), ('enough', 1), ('fast', 1), ('hitting', 1), ('narrowed', 1), ('ourselves', 1), ('bisected', 1), ('second', 1), ('road—no', 1), ('stop', 1), ('signs', 1), ('stoplight—and', 1), ('Nowhere', 1), ('sign', 1), ('indicated', 1), ('sharp', 1), ('curve', 1), ('steep', 1), ('drop-off', 1), ('own', 1), ('stopped', 1), ('dogs', 1), ('came', 1), ('running', 1), ('spring', 1), ('2011', 1), ('Tunisia', 1), ('Egypt', 1), ('Yemen', 1), ('made', 1), ('Daraa', 1), ('By', 1), ('marching', 1), ('nationwide', 1), ('April', 1), ('rare', 1), ('many', 1), ('bloodbaths', 1), ('Bashar', 1), ('Al-Assad', 1), ('fired', 1), ('unarmed', 1), ('young', 1), ('surviving', 1), ('ones', 1), ('pick', 1), ('filled', 1), ('try', 1), ('pretend', 1), ('happening', 1), ('walk', 1), ('six', 1), ('leafy', 1), ('overhung', 1), ('bougainvillea', 1), ('Homs', 1), ('Hama', 1), ('Aleppo', 1), ('At', 1), ('glance', 1), ('tell', 1), ('code', 1), ('license', 1), ('plate', 1), ('empty', 1), ('apartment', 1), ('snatched', 1), ('arrived', 1), ('son', 1), ('maid', 1), ('Both', 1), ('vehicles', 1), ('Damascus', 1), ('Each', 1), ('flew', 1), ('darkness', 1), ('hugged', 1), ('water', 1), ('string', 1), ('lights', 1), ('necklace', 1), ('tendrils', 1), ('color', 1), ('snaked', 1), ('those', 1), ('predicted', 1), ('fall', 1), ('within', 1), ('being', 1), ('changed', 1), ('daily', 1), ('flowing', 1), ('Returning', 1), ('intense', 1), ('Settling', 1), ('ask', 1), ('OK', 1), ('Tires', 1), ('fire', 1), ('Any', 1), ('roadblocks', 1), ('Should', 1), ('buy', 1), ('extra', 1), ('milk', 1), ('Back', 1), ('home', 1), ('prayer', 1), ('rang', 1), ('three', 1), ('different', 1), ('mosques', 1), ('bent', 1), ('pray', 1), ('cut', 1), ('scarred', 1), ('experience', 1), ('years', 1), ('rule', 1), ('gravity', 1), ('women', 1), ('forbidden', 1), ('deputize', 1), ('14-year-old', 1), ('sons', 1), ('cousins', 1), ('pilot', 1), ('often', 1), ('massive', 1), ('SUV', 1), ('gripped', 1), ('edge', 1), ('fatalities', 1), ('higher', 1), ('world', 1), ('Additionally', 1), ('demand', 1), ('eastern', 1), ('oil', 1), ('spent', 1), ('terrifying', 1), ('gritting', 1), ('teeth', 1), ('astonished', 1), ('blasting', 1), ('us', 1), ('120', 1), ('mph', 1), ('death', 1), ('wish', 1), ('ocean', 1), ('sand', 1), ('after', 1), ('while', 1), ('pushed', 1), ('required', 1), ('special', 1), ('permission', 1), ('UN', 1), ('managed', 1), ('Israel', 1), ('Hezbollah', 1), ('de', 1), ('facto', 1), ('government', 1), ('parts', 1), ('phone', 1), ('guys', 1), ('random', 1), ('Sunni', 1), ('brokers', 1), ('unlike', 1), ('proxy', 1), ('capital', 1), ('rich', 1), ('staggering', 1), ('condo', 1), ('buildings', 1), ('guards', 1), ('preferred', 1), ('foot', 1), ('finding', 1), ('evidence', 1), ('gunshots', 1), ('RPG', 1), ('September', 1), ('1982', 1), ('700', 1), ('civilians', 1), ('slaughtered', 1), ('refugee', 1), ('camps', 1), ('blocks', 1), ('Marine', 1), ('barracks', 1), ('300', 1), ('morning', 1), ('1983', 1), ('Our', 1), ('house', 1), ('embassy', 1), ('charred', 1), ('hostile', 1), ('takeover', 1), ('Every', 1), ('now', 1), ('shot-up', 1), ('hulk', 1), ('notorious', 1), ('opened', 1), ('1974', 1), ('operated', 1), ('overrun', 1), ('so-called', 1), ('hotels', 1), ('rival', 1), ('factions', 1), ('slaughtering', 1), ('rooms', 1), ('various', 1), ('downtown', 1), ('towers', 1), ('Now', 1), ('gutted', 1), ('shell', 1), ('topped', 1), ('revolving', 1), ('restaurant', 1), ('lately', 1), ('hosted', 1), ('birds', 1), ('trees', 1), ('rules', 1), ('Occasionally', 1), ('living', 1), ('so', 1), ('close', 1), ('together', 1), ('order', 1), ('beside', 1), ('secondary', 1), ('little', 1), ('compromises', 1), ('actually', 1), ('governed', 1), ('life', 1), ('execution', 1), ('gates', 1), ('ghostly', 1), ('community', 1), ('organism', 1), ('predisposed', 1), ('letting', 1), ('doing', 1), ('bad', 1), ('Instead', 1), ('matter', 1), ('habit', 1), ('public', 1), ('parks', 1), ('but', 1), ('general', 1), ('idea', 1), ('sharing', 1), ('preposterous', 1), ('Better', 1), ('tall', 1), ('walls', 1), ('bars', 1), ('battered', 1), ('Senaya', 1), ('Park', 1), ('green', 1), ('line', 1), ('separated', 1), ('east', 1), ('playground', 1), ('until', 1), ('recently', 1), ('sleep', 1), ('whose', 1), ('homes', 1), ('blown', 1), ('Even', 1), ('insisted', 1), ('walking', 1), ('everywhere', 1), ('leaving', 1), ('party', 1), ('90', 1), ('early', 1), ('pounding', 1), ('arriving', 1), ('sweat', 1), ('sitting', 1), ('most', 1), ('find', 1), ('far', 1), ('possible', 1), ('seven-hour', 1), ('shoot-out', 1), ('army', 1), ('journalist', 1), ('put', 1), ('helmet', 1), ('grabbed', 1), ('press', 1), ('ran', 1), ('shooting', 1), ('Stealthy', 1), ('leather', 1), ('jackets', 1), ('waist-holstered', 1), ('pistols', 1), ('appeared', 1), ('demanding', 1), ('identification', 1), ('Meanwhile', 1), ('others', 1), ('emerged', 1), ('held', 1), ('AK-47s', 1), ('air', 1), ('Young', 1), ('stood', 1), ('uneasily', 1), ('armored', 1), ('personnel', 1), ('carrier', 1), ('watching', 1), ('waiting', 1), ('exactly', 1), ('stake', 1), ('Later', 1), ('grenade', 1), ('seven', 1), ('This', 1), ('past', 1), ('summer', 1), ('foreign', 1), ('journalists', 1), ('south', 1), ('barrage', 1), ('rockets', 1), ('suburb', 1), ('season', 1), ('explosions', 1), ('More', 1), ('dozen', 1), ('guests', 1), ('gathering', 1), ('wedding', 1), ('photographer', 1), ('Kabul', 1), ('correspondent', 1), ('hammocks', 1), ('blue', 1), ('Let', 1), ('enjoy', 1), ('themselves', 1), ('smoke', 1), ('damage', 1), ('close—maybe', 1), ('six-lane', 1), ('Yet', 1), ('spare', 1), ('truth', 1), ('focused', 1), ('known', 1), ('un-hit', 1), ('ordered', 1), ('cocktail', 1), ('Whatever', 1), ('cost', 1), ('ways', 1), ('dollars', 1), ('least', 1), ('ride', 1), ('center', 1), ('$', 1), ('13', 1), ('August', 1), ('returned', 1), ('United', 1), ('States', 1), ('new', 1), ('job', 1), ('beckoned', 1), ('Los', 1), ('Angeles', 1), ('First', 1), ('Boston', 1), ('hopped', 1), ('station', 1), ('discussing', 1), ('chemical', 1), ('weapons', 1), ('Grinding', 1), ('gears', 1), ('wheel', 1), ('heading', 1), ('ache', 1), ('dense', 1), ('dangerous', 1), ('shuddering', 1), ('slow', 1), ('sarin', 1), ('gas', 1), ('eve', 1), ('apparent', 1), ('strikes', 1), ('much', 1), ('gone', 1), ('feeling', 1), ('missiles', 1), ('layer', 1), ('Setting', 1), ('cruise', 1), ('control', 1), ('limits', 1), ('kindness', 1), ('assert', 1), ('prerogative', 1), ('Nearly', 1), ('found', 1), ('abandoned', 1), ('Ranch', 1), ('encountered', 1), ('Kuwaitis', 1), ('lonely', 1), ('ahead', 1), ('dashing', 1), ('both', 1), ('plans', 1), ('Yosemite', 1), ('map', 1), ('Deep', 1), ('Springs', 1), ('screwed', 1), ('brandished', 1), ('northerly', 1), ('bypass', 1), ('Tonopah', 1), ('path', 1), ('myself', 1), ('did', 1), ('want', 1), ('nodded', 1), ('uncomprehending', 1), ('overwhelmed', 1), ('missed', 1), ('insane', 1), ('standing', 1), ('front', 1), ('flee', 1), ('white', 1), ('van', 1), ('sticking', 1), ('window', 1), ('voluminous', 1), ('desire', 1), ('sat', 1), ('Kuwaiti', 1), ('Her', 1), ('husband', 1), ('sympathy', 1), ('horror', 1), ('perhaps', 1), ('thanks', 1), ('car-load', 1), ('20s', 1), ('argued', 1), ('sharply', 1), ('Arabic', 1), ('One', 1), ('eyebrows', 1), ('furrowed', 1), ('pointed', 1), ('What', 1), ('border.', 1), ('Can', 1), ('follow', 1), ('asked', 1), ('eyes', 1), ('pleading', 1), ('don', 1), ('know', 1), ('going', 1), ('stick', 1), ('think', 1), ('reality', 1), ('believe', 1), ('do', 1), ('imagined', 1), ('barricades', 1), ('settling', 1), ('slip', 1), ('giant', 1), ('yawning', 1), ('chasm', 1), ('Old', 1), ('mattresses', 1), ('stacked', 1), ('hallway', 1), ('lined', 1), ('bedrooms', 1), ('poked', 1), ('shut', 1), ('Outside', 1), ('flood', 1), ('sunshine', 1), ('glad', 1), ('space', 1), ('roam', 1), ('part', 1), ('stuck', 1), ('bracing', 1), ('hoped', 1), ('wouldn', 1), ('pressed', 1), ('again', 1), ('host', 1), ('talking', 1), ('bullet-proof', 1), ('insert', 1), ('available', 1), ('kids', 1), ('backpacks', 1), ('Battleships', 1), ('Mediterranean', 1), ('Russia', 1), ('U.S.', 1), ('pretending', 1), ('didn', 1), ('Kansas', 1), ('(', 1), ('Missouri', 1), (')', 1), ('passed', 1), ('decommissioned', 1), ('plane', 1), ('camo', 1), ('paint', 1), ('flaking', 1), ('off', 1), ('machine', 1), ('last', 1), ('aid', 1), ('need', 1), ('A-10', 1), ('fighter', 1), ('jet', 1), ('accidentally', 1), ('dropped', 1), ('inert', 1), ('practice', 1), ('Maryland', 1), ('barkeep', 1), ('precisely', 1), ('outdoor', 1), ('surveillance', 1), ('camera', 1), ('impact', 1), ('No', 1), ('noticed', 1), ('Customers', 1), ('busy', 1), ('music', 1), ('loud', 1)]\n",
      "Sentiment Analysis Results for Document 1:\n",
      "Positive Score: 0.065\n",
      "Negative Score: 0.071\n",
      "Neutral Score: 0.864\n",
      "Compound Score: -0.4031\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "document_content = corpus.raw('document1.txt')\n",
    "words = word_tokenize(document_content)\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "# Print the most common words in the document\n",
    "print(fdist.most_common(1000))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sia.polarity_scores(document_content)\n",
    "\n",
    "# Print sentiment analysis results\n",
    "print(\"Sentiment Analysis Results for Document 1:\")\n",
    "print(\"Positive Score:\", sentiment_scores['pos'])\n",
    "print(\"Negative Score:\", sentiment_scores['neg'])\n",
    "print(\"Neutral Score:\", sentiment_scores['neu'])\n",
    "print(\"Compound Score:\", sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a9e905d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: “What is this?”\r\n",
      "\r\n",
      "“That,” I said, “is the border.”\r\n",
      "\r\n",
      "“Can we follow you?” the woman asked, her eyes pleading.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocess the content of Document 1\n",
    "document_content = corpus.raw('document1.txt')\n",
    "sentences = sent_tokenize(document_content)\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "\n",
    "# Process and tokenize the user's question\n",
    "user_question = \"What is this\"\n",
    "question_words = word_tokenize(user_question.lower())\n",
    "\n",
    "# Calculate TF-IDF vectors for the question and document sentences\n",
    "question_vector = tfidf.transform([user_question])\n",
    "cosine_scores = cosine_similarity(question_vector, tfidf_matrix)\n",
    "\n",
    "# Find the most relevant sentence in the document\n",
    "most_relevant_sentence_idx = cosine_scores.argmax()\n",
    "most_relevant_sentence = sentences[most_relevant_sentence_idx]\n",
    "\n",
    "# Present the answer to the user\n",
    "print(\"Answer:\", most_relevant_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa70ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts is already in the PATH environment variable.\n",
      "Updated PATH: C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\ProgramData\\anaconda3;C:\\ProgramData\\anaconda3\\Library\\mingw-w64\\bin;C:\\ProgramData\\anaconda3\\Library\\usr\\bin;C:\\ProgramData\\anaconda3\\Library\\bin;C:\\ProgramData\\anaconda3\\Scripts;C:\\Program Files\\Python311\\Scripts\\;C:\\Program Files\\Python311\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files\\dotnet\\;C:\\Users\\anas_\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\anas_\\.dotnet\\tools;C:\\Users\\anas_\\AppData\\Local\\GitHubDesktop\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Replace this with the path you want to add to the PATH environment variable\n",
    "new_path = r'C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts'\n",
    "\n",
    "# Check if the path already exists in PATH\n",
    "if new_path not in os.environ['PATH']:\n",
    "    # Append the new path to the PATH variable\n",
    "    os.environ['PATH'] = new_path + os.pathsep + os.environ['PATH']\n",
    "\n",
    "    # Notify the user\n",
    "    print(f'Added {new_path} to the PATH environment variable.')\n",
    "\n",
    "else:\n",
    "    # Notify the user that the path already exists in PATH\n",
    "    print(f'{new_path} is already in the PATH environment variable.')\n",
    "\n",
    "# You can verify the changes by printing the updated PATH\n",
    "print('Updated PATH:', os.environ['PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed485591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the 'nltk' module: C:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Get the path of the 'nltk' module\n",
    "nltk_module_path = nltk.__file__\n",
    "\n",
    "# Print the path\n",
    "print(\"Path to the 'nltk' module:\", nltk_module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08a00196",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(spacy_module_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can import 'spacy' from the specified path\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "\n",
    "# Add the path to the 'spacy' module\n",
    "spacy_module_path = r'C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts'\n",
    "\n",
    "# Append the path to sys.path\n",
    "sys.path.append(spacy_module_path)\n",
    "\n",
    "# Now you can import 'spacy' from the specified path\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy for advanced text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess the content of Document 1\n",
    "document_content = corpus.raw('document1.txt')\n",
    "sentences = sent_tokenize(document_content)\n",
    "\n",
    "# Define a function for advanced text processing\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize words and remove stopwords\n",
    "    preprocessed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return preprocessed_text\n",
    "\n",
    "# Preprocess document sentences\n",
    "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "# Process and tokenize the user's question\n",
    "user_question = \"What is Document 1 about?\"\n",
    "question_words = word_tokenize(user_question.lower())\n",
    "preprocessed_question = preprocess_text(user_question)\n",
    "\n",
    "# Calculate TF-IDF vectors for the question and document sentences\n",
    "tfidf = TfidfVectorizer()\n",
    "question_vector = tfidf.fit_transform([preprocessed_question])\n",
    "document_vectors = tfidf.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# Calculate cosine similarity scores\n",
    "cosine_scores = cosine_similarity(question_vector, document_vectors)\n",
    "\n",
    "# Find the most relevant sentence in the document\n",
    "most_relevant_sentence_idx = cosine_scores.argmax()\n",
    "most_relevant_sentence = sentences[most_relevant_sentence_idx]\n",
    "\n",
    "# Implement document expansion (additional context)\n",
    "expanded_document = \" \".join(sentences[max(0, most_relevant_sentence_idx - 2):most_relevant_sentence_idx + 3])\n",
    "\n",
    "# Present the answer to the user\n",
    "print(\"Answer:\", most_relevant_sentence)\n",
    "print(\"Expanded Context:\", expanded_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a71d70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\anas_\\\\OneDrive\\\\Python_Backup',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\anas_\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\anas_\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\Scripts',\n",
       " 'C:\\\\Users\\\\anas_\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\Scripts']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
