{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c9f6eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Positive\n",
      "Positive Score: 0.189\n",
      "Negative Score: 0.0\n",
      "Neutral Score: 0.811\n",
      "Compound Score: 0.4215\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "#nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Sample text for sentiment analysis\n",
    "text = \"one man is in the home. He is waiting to kiss his wife\"\n",
    "\n",
    "# Analyze the sentiment of the text\n",
    "sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "# Determine sentiment based on the compound score\n",
    "compound_score = sentiment_scores['compound']\n",
    "\n",
    "if compound_score >= 0.05:\n",
    "    sentiment = 'Positive'\n",
    "elif compound_score <= -0.05:\n",
    "    sentiment = 'Negative'\n",
    "else:\n",
    "    sentiment = 'Neutral'\n",
    "\n",
    "# Display the sentiment and scores\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Positive Score:\", sentiment_scores['pos'])\n",
    "print(\"Negative Score:\", sentiment_scores['neg'])\n",
    "print(\"Neutral Score:\", sentiment_scores['neu'])\n",
    "print(\"Compound Score:\", sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3829a5a",
   "metadata": {},
   "source": [
    "# 1. NLTK\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "Sentence Tokenization:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"This is a sample sentence. And here's another one.\"\n",
    "sentences = sent_tokenize(text)\n",
    "Word Tokenization:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = \"Tokenize this sentence into words.\"\n",
    "words = word_tokenize(text)\n",
    "2. Part-of-Speech Tagging:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import pos_tag\n",
    "words = word_tokenize(\"This is an example sentence.\")\n",
    "tagged_words = pos_tag(words)\n",
    "3. Named Entity Recognition (NER):\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import ne_chunk\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "named_entities = ne_chunk(tagged_words)\n",
    "4. Sentiment Analysis:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sia.polarity_scores(\"I love this product!\")\n",
    "5. Text Classification:\n",
    "\n",
    "Assuming you have a labeled dataset, you can train and evaluate a classifier using NLTK. For instance, a Naive Bayes classifier:\n",
    "python\n",
    "Copy code\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "training_data = [(feature_set, label) for feature_set, label in your_training_data]\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "6. WordNet Integration:\n",
    "\n",
    "Access synonyms and antonyms using WordNet in NLTK:\n",
    "python\n",
    "Copy code\n",
    "from nltk.corpus import wordnet\n",
    "synonyms = wordnet.synsets(\"happy\")\n",
    "antonyms = wordnet.synsets(\"happy\", antonyms=True)\n",
    "7. Frequency Distribution:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from nltk import FreqDist\n",
    "words = word_tokenize(\"This is a simple example sentence.\")\n",
    "freq_dist = FreqDist(words)\n",
    "8. Regular Expressions:\n",
    "\n",
    "You can use Python's regular expressions in combination with NLTK for more advanced text processing. For example, to find all capitalized words:\n",
    "python\n",
    "Copy code\n",
    "import re\n",
    "text = \"Find All Capitalized Words in This Text.\"\n",
    "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text)\n",
    "9. Text Summarization:\n",
    "\n",
    "NLTK provides some tools for extracting sentences based on criteria like sentence importance, which can be used in text summarization. However, more advanced summarization techniques may require additional libraries or algorithms.\n",
    "Please note that the above snippets provide a basic introduction to these NLP tasks using NLTK. Depending on your specific use case and requirements, you may need to extend these examples or explore other NLTK functions and modules in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5298780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how are you doing?', 'I was waiting for you.', 'I will be going soon']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "#nltk.download('punkt')\n",
    "\n",
    "text =\"how are you doing? I was waiting for you. I will be going soon\"\n",
    "sent = sent_tokenize(text)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ada95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['how', 'are', 'you', 'doing', '?', 'I', 'was', 'waiting', 'for', 'you', '.', 'I', 'will', 'be', 'going', 'soon']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"how are you doing? I was waiting for you. I will be going soon\"\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc72114a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('how', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('?', '.'), ('I', 'PRP'), ('was', 'VBD'), ('waiting', 'VBG'), ('for', 'IN'), ('you', 'PRP'), ('.', '.'), ('I', 'PRP'), ('will', 'MD'), ('be', 'VB'), ('going', 'VBG'), ('soon', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "    \n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "print(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee834806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Barack/NNP)\n",
      "  (PERSON Obama/NNP)\n",
      "  was/VBD\n",
      "  born/VBN\n",
      "  in/IN\n",
      "  (GPE Hawaii/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "text = \"Barack Obama was born in Hawaii.\"\n",
    "words = word_tokenize(text)\n",
    "tagged_words = pos_tag(words)\n",
    "named_entities = ne_chunk(tagged_words)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116cc989",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (554952159.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    NameError: name 'your_training_data' is not defined\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "training_data = [(feature_set, label) for feature_set, label in your_training_data]\n",
    "classifier = NaiveBayesClassifier.train(training_data)\n",
    "print(classifier)\n",
    "NameError: name 'your_training_data' is not defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2869c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anas_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms: {'deplorable', 'distressing', 'sad', 'pitiful', 'sorry', 'lamentable'}\n",
      "Antonyms: {'unhappy'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Find synonyms for the word \"happy\"\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets(\"sad\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "\n",
    "# Find antonyms for the synonyms\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"happy\"):\n",
    "    for lemma in syn.lemmas():\n",
    "        for antonym in lemma.antonyms():\n",
    "            antonyms.append(antonym.name())\n",
    "\n",
    "print(\"Synonyms:\", set(synonyms))\n",
    "print(\"Antonyms:\", set(antonyms))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15ef791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'example', 'sentence', '.']\n",
      "<FreqDist with 7 samples and 7 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize  # Import word_tokenize from nltk.tokenize\n",
    "\n",
    "text = \"This is a simple example sentence.\"\n",
    "words = word_tokenize(text)  # Tokenize the text\n",
    "freq_dist = FreqDist(words)  # Create a frequency distribution\n",
    "print(words)\n",
    "print(freq_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ba67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green Investment Appraisal (GIA) is another emerging approach that explicitly incorporates sustainability criteria into investment decisions. Integrated Cost-Benefit Analysis (CBA) combines financial and non-financial aspects, such as social and environmental impacts, to provide a holistic assessment of investment opportunities.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np  # Import NumPy\n",
    "\n",
    "def extractive_summarization(text, num_sentences=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "    \n",
    "    # Convert the 'scores' array to a 1D array or list\n",
    "    scores = np.array(tfidf_matrix.sum(axis=1)).ravel()  # Using NumPy 'ravel()'\n",
    "\n",
    "    # Alternatively, you can use 'tolist()' to convert to a list:\n",
    "    # scores = tfidf_matrix.sum(axis=1).tolist()\n",
    "\n",
    "    ranked_sentences = [sentences[i] for i in scores.argsort()[-num_sentences:]]\n",
    "    summary = ' '.join(ranked_sentences)\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "text = \"Integrated Cost-Benefit Analysis (CBA) combines financial and non-financial aspects, such as social and environmental impacts, to provide a holistic assessment of investment opportunities. Green Investment Appraisal (GIA) is another emerging approach that explicitly incorporates sustainability criteria into investment decisions. \"\n",
    "summary = extractive_summarization(text)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ef06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: hi\n",
      "Chatbot: Hello!\n",
      "You: exit\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# Greeting responses\n",
    "greetings = [\"hello\", \"hi\", \"hey\", \"howdy\", \"greetings\"]\n",
    "# Responses\n",
    "responses = [\"Hello!\", \"Hi there!\", \"Greetings!\", \"Hey!\", \"Hello, how can I help you today?\"]\n",
    "\n",
    "# Lemmatization function\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in text]\n",
    "\n",
    "# Function to respond to user input\n",
    "def respond(user_input):\n",
    "    user_words = lemmatize(word_tokenize(user_input.lower()))\n",
    "    for word in user_words:\n",
    "        if word in greetings:\n",
    "            return random.choice(responses)\n",
    "    return \"I'm sorry, I don't understand.\"\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "    response = respond(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f5ed55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c21df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_root = 'C:\\\\Users\\\\anas_\\\\OneDrive\\\\Python_Backup'  # Replace with the actual path to your corpus directory\n",
    "corpus = PlaintextCorpusReader(corpus_root, 'document1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e11f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['document1.txt']\n"
     ]
    }
   ],
   "source": [
    "file_ids = corpus.fileids()\n",
    "print(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76656e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_content = corpus.raw('document1.txt')\n",
    "document_words = corpus.words('document1.txt')\n",
    "document_sentences = corpus.sents('document1.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4761701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "document_content = corpus.raw('document1.txt')\n",
    "words = word_tokenize(document_content)\n",
    "fdist = FreqDist(words)\n",
    "\n",
    "# Print the most common words in the document\n",
    "#print(fdist.most_common(1000))\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = sia.polarity_scores(document_content)\n",
    "\n",
    "# Print sentiment analysis results\n",
    "#print(\"Sentiment Analysis Results for Document 1:\")\n",
    "#print(\"Positive Score:\", sentiment_scores['pos'])\n",
    "#print(\"Negative Score:\", sentiment_scores['neg'])\n",
    "#print(\"Neutral Score:\", sentiment_scores['neu'])\n",
    "#print(\"Compound Score:\", sentiment_scores['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9e905d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask me any Question According to Document 1: \n",
      "Question: Egypt and Yemen and finally made their way to?\n",
      "Answer: In the spring of 2011, protests that began in Tunisia spread to Egypt and Yemen and finally made their way to a town in Syria called Daraa.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Preprocess the content of Document 1\n",
    "document_content = corpus.raw('document1.txt')\n",
    "sentences = sent_tokenize(document_content)\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "\n",
    "# Process and tokenize the user's question\n",
    "#user_question = \"in the month we return to US?\"\n",
    "print(\"Ask me any Question According to Document 1: \")\n",
    "user_question = input(\"Question: \")\n",
    "question_words = word_tokenize(user_question.lower())\n",
    "\n",
    "# Calculate TF-IDF vectors for the question and document sentences\n",
    "question_vector = tfidf.transform([user_question])\n",
    "cosine_scores = cosine_similarity(question_vector, tfidf_matrix)\n",
    "\n",
    "# Find the most relevant sentence in the document\n",
    "most_relevant_sentence_idx = cosine_scores.argmax()\n",
    "most_relevant_sentence = sentences[most_relevant_sentence_idx]\n",
    "\n",
    "# Present the answer to the user\n",
    "print(\"Answer:\", most_relevant_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa70ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts is already in the PATH environment variable.\n",
      "Updated PATH: C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts;C:\\ProgramData\\anaconda3;C:\\ProgramData\\anaconda3\\Library\\mingw-w64\\bin;C:\\ProgramData\\anaconda3\\Library\\usr\\bin;C:\\ProgramData\\anaconda3\\Library\\bin;C:\\ProgramData\\anaconda3\\Scripts;C:\\Program Files\\Python311\\Scripts\\;C:\\Program Files\\Python311\\;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Windows\\System32\\OpenSSH\\;C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn\\;C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\;C:\\Program Files\\dotnet\\;C:\\Users\\anas_\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\anas_\\.dotnet\\tools;C:\\Users\\anas_\\AppData\\Local\\GitHubDesktop\\bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Replace this with the path you want to add to the PATH environment variable\n",
    "new_path = r'C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts'\n",
    "\n",
    "# Check if the path already exists in PATH\n",
    "if new_path not in os.environ['PATH']:\n",
    "    # Append the new path to the PATH variable\n",
    "    os.environ['PATH'] = new_path + os.pathsep + os.environ['PATH']\n",
    "\n",
    "    # Notify the user\n",
    "    print(f'Added {new_path} to the PATH environment variable.')\n",
    "\n",
    "else:\n",
    "    # Notify the user that the path already exists in PATH\n",
    "    print(f'{new_path} is already in the PATH environment variable.')\n",
    "\n",
    "# You can verify the changes by printing the updated PATH\n",
    "print('Updated PATH:', os.environ['PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed485591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to the 'nltk' module: C:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Get the path of the 'nltk' module\n",
    "nltk_module_path = nltk.__file__\n",
    "\n",
    "# Print the path\n",
    "print(\"Path to the 'nltk' module:\", nltk_module_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a00196",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(spacy_module_path)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Now you can import 'spacy' from the specified path\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize, sent_tokenize\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "\n",
    "# Add the path to the 'spacy' module\n",
    "spacy_module_path = r'C:\\Users\\anas_\\AppData\\Roaming\\Python\\Python311\\Scripts'\n",
    "\n",
    "# Append the path to sys.path\n",
    "sys.path.append(spacy_module_path)\n",
    "\n",
    "# Now you can import 'spacy' from the specified path\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load spaCy for advanced text processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Preprocess the content of Document 1\n",
    "document_content = corpus.raw('document1.txt')\n",
    "sentences = sent_tokenize(document_content)\n",
    "\n",
    "# Define a function for advanced text processing\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize words and remove stopwords\n",
    "    preprocessed_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
    "    return preprocessed_text\n",
    "\n",
    "# Preprocess document sentences\n",
    "preprocessed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "\n",
    "# Process and tokenize the user's question\n",
    "user_question = \"What is Document 1 about?\"\n",
    "question_words = word_tokenize(user_question.lower())\n",
    "preprocessed_question = preprocess_text(user_question)\n",
    "\n",
    "# Calculate TF-IDF vectors for the question and document sentences\n",
    "tfidf = TfidfVectorizer()\n",
    "question_vector = tfidf.fit_transform([preprocessed_question])\n",
    "document_vectors = tfidf.fit_transform(preprocessed_sentences)\n",
    "\n",
    "# Calculate cosine similarity scores\n",
    "cosine_scores = cosine_similarity(question_vector, document_vectors)\n",
    "\n",
    "# Find the most relevant sentence in the document\n",
    "most_relevant_sentence_idx = cosine_scores.argmax()\n",
    "most_relevant_sentence = sentences[most_relevant_sentence_idx]\n",
    "\n",
    "# Implement document expansion (additional context)\n",
    "expanded_document = \" \".join(sentences[max(0, most_relevant_sentence_idx - 2):most_relevant_sentence_idx + 3])\n",
    "\n",
    "# Present the answer to the user\n",
    "print(\"Answer:\", most_relevant_sentence)\n",
    "print(\"Expanded Context:\", expanded_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a71d70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\anas_\\\\OneDrive\\\\Python_Backup',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\python310.zip',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\DLLs',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\anaconda3',\n",
       " '',\n",
       " 'C:\\\\Users\\\\anas_\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\win32',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib',\n",
       " 'C:\\\\ProgramData\\\\anaconda3\\\\lib\\\\site-packages\\\\Pythonwin',\n",
       " 'C:\\\\Users\\\\anas_\\\\AppData\\\\Roaming\\\\Python\\\\Python311\\\\Scripts']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d784208b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anas_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[1;32m---> 20\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# User's question\u001b[39;00m\n\u001b[0;32m     23\u001b[0m user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the average price of items in category A?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_column'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data11.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "data['processed_text'] = data['text_column'].apply(preprocess_text)\n",
    "\n",
    "# User's question\n",
    "user_question = \"What is the average price of items in category A?\"\n",
    "\n",
    "# Process and analyze the user's question\n",
    "\n",
    "# Find relevant rows using TF-IDF or other methods\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['processed_text'])\n",
    "question_vector = tfidf.transform([preprocess_text(user_question)])\n",
    "similarity_scores = (tfidf_matrix * question_vector.T).toarray()\n",
    "\n",
    "# Find the most relevant rows based on the similarity scores\n",
    "top_k = 5  # Number of top documents to retrieve\n",
    "top_indices = similarity_scores.argsort(axis=0)[-top_k:][::-1]\n",
    "\n",
    "# Retrieve information from relevant rows and construct an answer\n",
    "answers = []\n",
    "for idx in top_indices:\n",
    "    answers.append(data.iloc[idx]['text_column'])\n",
    "\n",
    "# Print or return the answers\n",
    "for answer in answers:\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a195fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anas_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m text_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndustry_name_NZSIOC\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariable_name\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariable_category\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[1;32m---> 23\u001b[0m     data[col] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# User's question\u001b[39;00m\n\u001b[0;32m     26\u001b[0m user_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat was the value of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVariable_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m in 2020 for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndustry_name_NZSIOC\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(tex)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(tex):\n\u001b[1;32m---> 15\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[0;32m     17\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[38;5;124;03m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentences_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrealign_boundaries\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentences_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, realign_boundaries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[38;5;124;03m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;124;03m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;124;03m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;124;03m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [text[s:e] \u001b[38;5;28;01mfor\u001b[39;00m s, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (sentence\u001b[38;5;241m.\u001b[39mstart, sentence\u001b[38;5;241m.\u001b[39mstop)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;124;03mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence1, sentence2 \u001b[38;5;129;01min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sentence1\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m realign, sentence1\u001b[38;5;241m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_slices_from_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m match, context \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_match_potential_end_contexts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1396\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(last_break, match\u001b[38;5;241m.\u001b[39mend())\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lang_vars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperiod_context_re\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinditer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)):\n\u001b[0;32m   1376\u001b[0m     \u001b[38;5;66;03m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matches \u001b[38;5;129;01mand\u001b[39;00m match\u001b[38;5;241m.\u001b[39mend() \u001b[38;5;241m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data11.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to multiple columns of interest\n",
    "text_columns = ['Year', 'Industry_name_NZSIOC', 'Variable_name','Variable_category','Value','Units']\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(preprocess_text)\n",
    "\n",
    "# User's question\n",
    "user_question = \"What was the value of 'Variable_name' in 2020 for 'Industry_name_NZSIOC'?\"\n",
    "\n",
    "# Process and analyze the user's question\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[text_columns].agg(' '.join, axis=1)\n",
    "\n",
    "# Find relevant rows using TF-IDF or other methods\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])\n",
    "question_vector = tfidf.transform([preprocess_text(user_question)])\n",
    "similarity_scores = (tfidf_matrix * question_vector.T).toarray()\n",
    "\n",
    "# Find the most relevant rows based on the similarity scores\n",
    "top_k = 5  # Number of top documents to retrieve\n",
    "top_indices = similarity_scores.argsort(axis=0)[-top_k:][::-1]\n",
    "\n",
    "# Retrieve information from relevant rows and construct an answer\n",
    "answers = []\n",
    "for idx in top_indices:\n",
    "    answers.append(data.iloc[idx]['combined_text'])\n",
    "\n",
    "# Print or return the answers\n",
    "for answer in answers:\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba47c2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13907    2018 industries interest dividends donations f...\n",
      "Name: combined_text, dtype: object\n",
      "13907    2018 industries interest dividends donations f...\n",
      "Name: combined_text, dtype: object\n",
      "13907    2018 industries interest dividends donations f...\n",
      "Name: combined_text, dtype: object\n",
      "13907    2018 industries interest dividends donations f...\n",
      "Name: combined_text, dtype: object\n",
      "13907    2018 industries interest dividends donations f...\n",
      "Name: combined_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data11.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(tex):\n",
    "    if isinstance(tex, str):  # Check if the data is a string\n",
    "        tokens = word_tokenize(tex)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    elif isinstance(tex, (int, float)):  # Check if the data is an int or float\n",
    "        return str(tex)  # Convert to string if it's numeric\n",
    "    else:\n",
    "        return \"\"  # Handle other data types by returning an empty string\n",
    "\n",
    "# Specify the names of columns containing text data\n",
    "text_columns = ['Year', 'Industry_name_NZSIOC', 'Variable_name','Variable_category','Value','Units']\n",
    "\n",
    "# Apply preprocessing to text columns\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(preprocess_text)\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[text_columns].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# User's question\n",
    "user_question = \"What was the value of 'Variable_name' in 2020 for 'Industry_name_NZSIOC'?\"\n",
    "\n",
    "# Process and analyze the user's question\n",
    "\n",
    "# Find relevant rows using TF-IDF or other methods\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])\n",
    "question_vector = tfidf.transform([preprocess_text(user_question)])\n",
    "similarity_scores = (tfidf_matrix * question_vector.T).toarray()\n",
    "\n",
    "# Find the most relevant rows based on the similarity scores\n",
    "top_k = 5  # Number of top documents to retrieve\n",
    "top_indices = similarity_scores.argsort(axis=0)[-top_k:][::-1]\n",
    "\n",
    "# Retrieve information from relevant rows and construct an answer\n",
    "answers = []\n",
    "for idx in top_indices:\n",
    "    answers.append(data.iloc[idx]['combined_text'])\n",
    "\n",
    "# Print or return the answersfor answer in answers:\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90aaeba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data11.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "#print(stop_words)\n",
    "#print(text)\n",
    "#print(word_tokenize(tex))\n",
    "\n",
    "def preprocess_text(tex):\n",
    "    if isinstance(tex, str):  # Check if the data is a string\n",
    "        tokens = word_tokenize(tex)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    elif isinstance(tex, (int, float)):  # Check if the data is an int or float\n",
    "        return str(tex)  # Convert to string if it's numeric\n",
    "    else:\n",
    "        return \"\"  # Handle other data types by returning an empty string\n",
    "\n",
    "    \n",
    "# Specify the names of columns containing text data\n",
    "text_columns = ['Year', 'Industry_name_NZSIOC', 'Variable_name','Variable_category','Value','Units']\n",
    "\n",
    "# Apply preprocessing to text columns\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(preprocess_text)\n",
    "   # print(data[col])\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[text_columns].astype(str).agg(' '.join, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d9d056d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Buffer has wrong number of dimensions (expected 1, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_indices:\n\u001b[1;32m---> 29\u001b[0m     answers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m answer \u001b[38;5;129;01min\u001b[39;00m answers:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1616\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1614\u001b[0m \u001b[38;5;66;03m# a list of integers\u001b[39;00m\n\u001b[0;32m   1615\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_list_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;66;03m# a single integer\u001b[39;00m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1620\u001b[0m     key \u001b[38;5;241m=\u001b[39m item_from_zerodim(key)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1587\u001b[0m, in \u001b[0;36m_iLocIndexer._get_list_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;124;03mReturn Series values by list or array of integers.\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \u001b[38;5;124;03m`axis` can only be zero.\u001b[39;00m\n\u001b[0;32m   1585\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;66;03m# re-raise with different error message\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositional indexers are out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3902\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, indices, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3896\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3900\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3901\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3902\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3903\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   3904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:3886\u001b[0m, in \u001b[0;36mNDFrame._take\u001b[1;34m(self, indices, axis, convert_indices)\u001b[0m\n\u001b[0;32m   3879\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3880\u001b[0m \u001b[38;5;124;03mInternal version of the `take` allowing specification of additional args.\u001b[39;00m\n\u001b[0;32m   3881\u001b[0m \n\u001b[0;32m   3882\u001b[0m \u001b[38;5;124;03mSee the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m-> 3886\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3888\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3891\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3892\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(new_data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:978\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify, convert_indices)\u001b[0m\n\u001b[0;32m    975\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    977\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 978\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:751\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mall_none(\u001b[38;5;241m*\u001b[39mnew_refs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    752\u001b[0m         blk\u001b[38;5;241m.\u001b[39mtake_nd(\n\u001b[0;32m    753\u001b[0m             indexer,\n\u001b[0;32m    754\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    755\u001b[0m             fill_value\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    756\u001b[0m                 fill_value \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mfill_value\n\u001b[0;32m    757\u001b[0m             ),\n\u001b[0;32m    758\u001b[0m         )\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py:752\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mall_none(\u001b[38;5;241m*\u001b[39mnew_refs) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 752\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    760\u001b[0m     ]\n\u001b[0;32m    761\u001b[0m     new_refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    762\u001b[0m     parent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:880\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m    877\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m#  this assertion\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m new_mgr_locs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\core\\array_algos\\take.py:163\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    158\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    160\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    161\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 163\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    166\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mpandas\\_libs\\algos_take_helper.pxi:2110\u001b[0m, in \u001b[0;36mpandas._libs.algos.take_2d_axis1_object_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Buffer has wrong number of dimensions (expected 1, got 2)"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# User's question\n",
    "user_question = \"What was the value of 'Total income' in 2016 for 'Agriculture, Forestry and Fishing'?\"\n",
    "\n",
    "# Process and analyze the user's question\n",
    "\n",
    "# Find relevant rows using TF-IDF or other methods\n",
    "tfidf = TfidfVectorizer(max_df=0.85, min_df=0.05, ngram_range=(1, 2))\n",
    "word2vec_model = Word2Vec(sentences=data['combined_text'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "threshold = 0.3\n",
    "top_indices = [i for i, score in enumerate(similarity_scores) if score > threshold]\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])\n",
    "normalized_tfidf_matrix = tfidf_matrix / tfidf_matrix.sum(axis=1)\n",
    "\n",
    "\n",
    "question_vector = tfidf.transform([preprocess_text(user_question)])\n",
    "similarity_scores = (normalized_tfidf_matrix * question_vector.T)\n",
    "#print(similarity_scores)\n",
    "\n",
    "top_k = 1  # Number of top documents to retrieve\n",
    "top_indices = similarity_scores.argsort(axis=0)[-top_k:][::-1]\n",
    "\n",
    "# Retrieve information from relevant rows and construct an answer\n",
    "answers = []\n",
    "x = []\n",
    "for idx in top_indices:\n",
    "    answers.append(data.iloc[idx]['combined_text'])\n",
    "    \n",
    "for answer in answers:\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bf0d3d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Year Industry_aggregation_NZSIOC Industry_code_NZSIOC  \\\n",
      "32                             Level 1                   AA   \n",
      "303                            Level 3                 AA21   \n",
      "334                            Level 4                AA211   \n",
      "395                            Level 3                 AA32   \n",
      "426                            Level 1                   BB   \n",
      "...    ...                         ...                  ...   \n",
      "41680                          Level 3                 ZZ11   \n",
      "41681                          Level 3                 ZZ11   \n",
      "41691                          Level 3                 ZZ11   \n",
      "41701                          Level 3                 ZZ11   \n",
      "41711                          Level 3                 ZZ11   \n",
      "\n",
      "                                    Industry_name_NZSIOC               Units  \\\n",
      "32                      agriculture forestry and fishing  Dollars (millions)   \n",
      "303                                 forestry and logging  Dollars (millions)   \n",
      "334                                 forestry and logging  Dollars (millions)   \n",
      "395    agriculture forestry and fishing support servi...  Dollars (millions)   \n",
      "426                                               mining  Dollars (millions)   \n",
      "...                                                  ...                 ...   \n",
      "41680                         food product manufacturing  Dollars (millions)   \n",
      "41681                         food product manufacturing  Dollars (millions)   \n",
      "41691                         food product manufacturing  Dollars (millions)   \n",
      "41701                         food product manufacturing  Dollars (millions)   \n",
      "41711                         food product manufacturing          Percentage   \n",
      "\n",
      "      Variable_code                         Variable_name  \\\n",
      "32              H04           sales of goods and services   \n",
      "303             H04           sales of goods and services   \n",
      "334             H04           sales of goods and services   \n",
      "395             H04           sales of goods and services   \n",
      "426             H04           sales of goods and services   \n",
      "...             ...                                   ...   \n",
      "41680           H02  sales of goods not further processed   \n",
      "41681           H03     sales of other goods and services   \n",
      "41691           H17  purchases of goods bought for resale   \n",
      "41701           H28             disposals of fixed assets   \n",
      "41711           H38   margin on sales of goods for resale   \n",
      "\n",
      "           Variable_category Value  \\\n",
      "32     financial performance         \n",
      "303    financial performance         \n",
      "334    financial performance         \n",
      "395    financial performance         \n",
      "426    financial performance         \n",
      "...                      ...   ...   \n",
      "41680  financial performance         \n",
      "41681  financial performance         \n",
      "41691  financial performance         \n",
      "41701     financial position         \n",
      "41711       financial ratios         \n",
      "\n",
      "                                  Industry_code_ANZSIC06  \\\n",
      "32                                   ANZSIC06 division A   \n",
      "303                                  ANZSIC06 group A030   \n",
      "334                                  ANZSIC06 group A030   \n",
      "395                 ANZSIC06 groups A042, A051, and A052   \n",
      "426                                  ANZSIC06 division B   \n",
      "...                                                  ...   \n",
      "41680  ANZSIC06 groups C111, C112, C113, C114, C115, ...   \n",
      "41681  ANZSIC06 groups C111, C112, C113, C114, C115, ...   \n",
      "41691  ANZSIC06 groups C111, C112, C113, C114, C115, ...   \n",
      "41701  ANZSIC06 groups C111, C112, C113, C114, C115, ...   \n",
      "41711  ANZSIC06 groups C111, C112, C113, C114, C115, ...   \n",
      "\n",
      "                                           combined_text  \n",
      "32      agriculture forestry and fishing sales of goo...  \n",
      "303     forestry and logging sales of goods and servi...  \n",
      "334     forestry and logging sales of goods and servi...  \n",
      "395     agriculture forestry and fishing support serv...  \n",
      "426     mining sales of goods and services financial ...  \n",
      "...                                                  ...  \n",
      "41680   food product manufacturing sales of goods not...  \n",
      "41681   food product manufacturing sales of other goo...  \n",
      "41691   food product manufacturing purchases of goods...  \n",
      "41701   food product manufacturing disposals of fixed...  \n",
      "41711   food product manufacturing margin on sales of...  \n",
      "\n",
      "[3465 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        #tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Specify the names of columns containing text data\n",
    "text_columns = ['Year','Level', 'Industry', 'Variable','Value']\n",
    "\n",
    "# Apply preprocessing to text columns\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].apply(preprocess_text)\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[text_columns].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# User's question\n",
    "user_question = \"What was the value of 'Depreciation' in 2018 for 'All industries'?\"\n",
    "\n",
    "# Process and analyze the user's question\n",
    "tfidf = TfidfVectorizer(max_df=0.85, min_df=0.05, ngram_range=(1, 2))\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])\n",
    "question_vector = tfidf.transform([preprocess_text(user_question)])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, question_vector)\n",
    "\n",
    "# Find relevant rows based on the cosine similarity scores\n",
    "cosine_similarity_threshold = 0.3\n",
    "relevant_rows = data[cosine_similarities.flatten() > cosine_similarity_threshold]\n",
    "\n",
    "# Print or return the relevant rows\n",
    "print(relevant_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "10297bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the task you want to perform: average value of all accomodation\n",
      "Result:\n",
      "Year                                                        2013.0\n",
      "Level                                                      Level 4\n",
      "Industry                                             Accommodation\n",
      "Variable                           Total income per employee count\n",
      "Value                                                      108,900\n",
      "Unnamed: 5                                                     NaN\n",
      "Unnamed: 6                                                     NaN\n",
      "Unnamed: 7                                                     NaN\n",
      "Unnamed: 8                                                     NaN\n",
      "Unnamed: 9                                                     NaN\n",
      "combined_text    2013.0 Level 4 Accommodation Total income per ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data111.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[['Year', 'Level', 'Industry', 'Variable', 'Value']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# User Input for Task\n",
    "user_input = input(\"Enter the task you want to perform: \")\n",
    "\n",
    "# Process and analyze the user's query\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf.fit_transform(data['combined_text'])\n",
    "user_query_vector = tfidf.transform([preprocess_text(user_input)])\n",
    "\n",
    "# Calculate cosine similarity between user input and data\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix, user_query_vector)\n",
    "\n",
    "# Find the most relevant row based on the cosine similarity scores\n",
    "most_similar_row_index = cosine_similarities.argmax()\n",
    "result_row = data.iloc[most_similar_row_index]\n",
    "\n",
    "# Display the result\n",
    "print(\"Result:\")\n",
    "print(result_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "05b20df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a query: find average of Accomodation\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sympify() got an unexpected keyword argument 'symbols'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sp\u001b[38;5;241m.\u001b[39mpreorder_traversal(parsed_expr, stop\u001b[38;5;241m=\u001b[39m[sp\u001b[38;5;241m.\u001b[39mSymbol, sp\u001b[38;5;241m.\u001b[39mPow, sp\u001b[38;5;241m.\u001b[39mMul, sp\u001b[38;5;241m.\u001b[39mAdd, sp\u001b[38;5;241m.\u001b[39mSub])\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Extract mathematical functions from the user input\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m math_functions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(node) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[43mextract_math_functions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_input\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Evaluate the mathematical functions using sympy\u001b[39;00m\n\u001b[0;32m     42\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[107], line 35\u001b[0m, in \u001b[0;36mextract_math_functions\u001b[1;34m(expression)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_math_functions\u001b[39m(expression):\n\u001b[0;32m     34\u001b[0m     symbols \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39msymbols(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx y z\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Define symbols for potential variables\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     parsed_expr \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msympify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymbols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sp\u001b[38;5;241m.\u001b[39mpreorder_traversal(parsed_expr, stop\u001b[38;5;241m=\u001b[39m[sp\u001b[38;5;241m.\u001b[39mSymbol, sp\u001b[38;5;241m.\u001b[39mPow, sp\u001b[38;5;241m.\u001b[39mMul, sp\u001b[38;5;241m.\u001b[39mAdd, sp\u001b[38;5;241m.\u001b[39mSub])\n",
      "\u001b[1;31mTypeError\u001b[0m: sympify() got an unexpected keyword argument 'symbols'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sympy as sp\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data111.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[['Year', 'Level', 'Industry', 'Variable', 'Value']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# User Input for Task\n",
    "user_input = input(\"Enter a query: \")\n",
    "\n",
    "# Process and analyze the user's query\n",
    "preprocessed_input = preprocess_text(user_input)\n",
    "\n",
    "# Identify mathematical functions using sympy\n",
    "def extract_math_functions(expression):\n",
    "    symbols = sp.symbols('x y z')  # Define symbols for potential variables\n",
    "    parsed_expr = sp.sympify(expression, symbols=symbols)\n",
    "    return sp.preorder_traversal(parsed_expr, stop=[sp.Symbol, sp.Pow, sp.Mul, sp.Add, sp.Sub])\n",
    "\n",
    "# Extract mathematical functions from the user input\n",
    "math_functions = [str(node) for node in extract_math_functions(preprocessed_input)]\n",
    "\n",
    "# Evaluate the mathematical functions using sympy\n",
    "results = {}\n",
    "for func in math_functions:\n",
    "    try:\n",
    "        expr = sp.sympify(func)\n",
    "        result = expr.evalf()  # Evaluate the expression\n",
    "        results[func] = result\n",
    "    except sp.SympifyError:\n",
    "        results[func] = \"Error: Unable to evaluate\"\n",
    "\n",
    "# Display the results\n",
    "print(\"Mathematical Functions Identified:\")\n",
    "for func in results:\n",
    "    print(f\"{func}: {results[func]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e1963949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a query: add 1 in 2\n",
      "Mathematical Expressions Identified:\n",
      "add: add\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import sympy as sp\n",
    "\n",
    "# Load your DataFrame\n",
    "data = pd.read_csv(\"data111.csv\")\n",
    "\n",
    "# Preprocess the text data\n",
    "#nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Combine the text from multiple columns into a single text column\n",
    "data['combined_text'] = data[['Year', 'Level', 'Industry', 'Variable', 'Value']].astype(str).agg(' '.join, axis=1)\n",
    "\n",
    "# User Input for Task\n",
    "user_input = input(\"Enter a query: \")\n",
    "\n",
    "# Process and analyze the user's query\n",
    "preprocessed_input = preprocess_text(user_input)\n",
    "\n",
    "# Identify mathematical expressions in words\n",
    "def identify_math_expressions(text):\n",
    "    math_expressions = []\n",
    "    words = word_tokenize(text)\n",
    "    current_expression = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in ['plus', 'minus', 'times', 'divided', 'by']:\n",
    "            math_expressions.append(' '.join(current_expression))\n",
    "            current_expression = []\n",
    "        else:\n",
    "            current_expression.append(word)\n",
    "\n",
    "    if current_expression:\n",
    "        math_expressions.append(' '.join(current_expression))\n",
    "\n",
    "    return math_expressions\n",
    "\n",
    "# Extract mathematical expressions from the user input\n",
    "expressions_in_words = identify_math_expressions(preprocessed_input)\n",
    "\n",
    "# Translate expressions to mathematical notation and evaluate using sympy\n",
    "results = {}\n",
    "for expression_in_words in expressions_in_words:\n",
    "    try:\n",
    "        # Translate words to mathematical notation\n",
    "        expression_in_math = expression_in_words.replace('plus', '+').replace('minus', '-').replace('times', '*').replace('divided by', '/')\n",
    "        # Evaluate the expression\n",
    "        result = sp.sympify(expression_in_math).evalf()\n",
    "        results[expression_in_words] = result\n",
    "    except sp.SympifyError:\n",
    "        results[expression_in_words] = \"Error: Unable to evaluate\"\n",
    "\n",
    "# Display the results\n",
    "print(\"Mathematical Expressions Identified:\")\n",
    "for expression_in_words, result in results.items():\n",
    "    print(f\"{expression_in_words}: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
